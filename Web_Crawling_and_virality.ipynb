{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News\n",
    "\n",
    "Crawling: Crawl news and information websites.\n",
    "Machine learning:\n",
    "Determine whether a given article is fake.\n",
    "Find out what category of news the article falls into.\n",
    "Predict the likelihood that the article will go viral.\n",
    "\n",
    "### Ecommerce\n",
    "Crawling: Crawl Ecommerce websites.\n",
    "Machine learning:\n",
    "Determine what colors and designs are in vogue, something that I’m sure marketers would kill for, using image recognition and review sentiment analysis.\n",
    "Predict which brands/retailers are doing well (I’ve worked on something similar), something that Wall Street could be in the market for.\n",
    "Find out product pages from across websites that refer to the same product, a key task for price comparison, using RNNs.\n",
    "\n",
    "### Music\n",
    "Crawling: Crawl lyrics sites to get lyrics of songs across the web.\n",
    "Machine learning:\n",
    "Use a generative model to auto-generate your own song lyrics (AI lyricist anybody?).\n",
    "Identify plagiarised songs by detecting common patterns in songs by different artists; build a Song2Vec perhaps?\n",
    "\n",
    "### Government\n",
    "Crawling: Pull datasets/data from government websites.\n",
    "Machine learning:\n",
    "Identify trends in climate change and predict future temperature rises, using regression and more.\n",
    "Predict population migration patters or population increases using historical data.\n",
    "### Books\n",
    "Crawling: Pull the content of publicly available books.\n",
    "Machine learning:\n",
    "Identify works of writing that may have been plagiarized (Book2Vec?).\n",
    "Auto-generate book summaries using Seq2Seq techniques (this is a bit out there considering the mature of technology today, but is perhaps one for the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look something interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online News Crawling and Virality Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will learn how to crawl news from popular news websites and then anticipate the likelihood of virality of that crawled news. Read the article till the end.\n",
    "\n",
    "Firstly, we will crawl the news from a popular news website of your choice. In my case, I crawled the Times of India website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install newspaper library from PyPI: (for python 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import all the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom newspaper import Article\\nimport pandas as pd\\nimport numpy as np\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set the URL for news website and start making request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://timesofindia.indiatimes.com/world\"r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Get the soup of articles on website using beautiful soup and fetch all the links present in soup in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nsoup = BeautifulSoup(r.content, 'html5lib')\\ntable = soup.findAll('a', attrs = {'class':'w_img'})\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "soup = BeautifulSoup(r.content, 'html5lib')\n",
    "table = soup.findAll('a', attrs = {'class':'w_img'})\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Download the article from the links using newspaper library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in news:\\narticle = Article(i, language=\"en\")\\narticle.download()\\narticle.parse()\\narticle.nlp()\\ndata={}\\ndata[\\'Title\\']=article.title\\ndata[\\'Text\\']=article.text\\ndata[\\'Summary\\']=article.summary\\ndata[\\'Keywords\\']=article.keywords\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in news:\n",
    "article = Article(i, language=\"en\")\n",
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()\n",
    "data={}\n",
    "data['Title']=article.title\n",
    "data['Text']=article.text\n",
    "data['Summary']=article.summary\n",
    "data['Keywords']=article.keywords\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We will make a regression model for predicting the virality of news\n",
    "\n",
    "1. Download the Online News Dataset from UCI Repository\n",
    "2. Load and split the data into x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfull_data = clean_cols(pd.read_csv(FILEPATH))\\ntrain_set, test_set = train_test_split(full_data, test_size=0.20, random_state=42)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "full_data = clean_cols(pd.read_csv(FILEPATH))\n",
    "train_set, test_set = train_test_split(full_data, test_size=0.20, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop non-predicting columns\n",
    "4. We will use RandomForest Regressor for training our regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclf = RandomForestRegressor(random_state=42)\\nclf.fit(x_train, y_train)\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "clf = RandomForestRegressor(random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is ready, Let’s convert our crawled news into the format of our prediction model because we cannot simply provide just the text, we have to find features from the news text so that they can help to predict the virality.\n",
    "Features will include:\n",
    "\n",
    "1. No. of words in title\n",
    "2. No. of words in content(news article)\n",
    "3. Rate of unique tokens\n",
    "4. Rate of non-stop words\n",
    "5. Rate of non-stop unique words\n",
    "6. No. of URL’s present in content\n",
    "7. No. of Images in content\n",
    "8. No. of Videos in content\n",
    "9. Average word length\n",
    "10. No. of unique keywords\n",
    "11. type of data channel: lifestyle, entertainment, business, technology, social media, world\n",
    "12. day of week\n",
    "13. Subjectivity\n",
    "14. Sentiment polarity\n",
    "15. Rate of negative words\n",
    "16. Rate of Positive words\n",
    "17. Average Negative polarity\n",
    "18. Average Positive Polarity\n",
    "19. Minimum Positive polarity\n",
    "20. Maximum Positive Polarity\n",
    "21. Maximum Negative Polarity\n",
    "22. Minimum Negative Polarity\n",
    "23. Title Subjectivity\n",
    "24. Title Sentiment Polarity\n",
    "\n",
    "Append all these features to a list and make a data frame out of it.\n",
    "\n",
    "Use this data frame to predict the virality of news using our regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an Example.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted information\n",
    "\n",
    "news-please extracts the following attributes from news articles. Also, have a look at an examplary json file extracted by news-please.\n",
    "\n",
    "1. headline\n",
    "2. lead paragraph\n",
    "3. main text\n",
    "4. main image\n",
    "5. name(s) of author(s)\n",
    "6. publication date\n",
    "7. language\n",
    "8. Features\n",
    "\n",
    "works out of the box: install with pip, add URLs of your pages, run :-)\n",
    "run news-please conveniently using its CLI mode\n",
    "use it as a library within your own software\n",
    "extract articles from commoncrawl.org's news archive\n",
    "\n",
    "### Modes and use cases\n",
    "\n",
    "news-please supports three main use cases, which are explained in more detail in the following.\n",
    "\n",
    "### CLI mode\n",
    "\n",
    "stores extracted results in JSON files, PostgreSQL, ElasticSearch, or your own storage\n",
    "simple but extensive configuration (if you want to tweak the results)\n",
    "revisions: crawl articles multiple times and track changes\n",
    "\n",
    "### Library mode\n",
    "\n",
    "crawl and extract information given a list of article URLs\n",
    "to use news-please within your own Python code\n",
    "\n",
    "News archive from commoncrawl.org\n",
    "commoncrawl.org provides an extensive, free-to-use archive of news articles from small and major publishers world wide\n",
    "news-please enables users to conveniently download and extract articles from commoncrawl.org\n",
    "you can optionally define filter criteria, such as news publisher(s) or the date period, within which articles need to be published\n",
    "\n",
    "clone the news-please repository, install the awscli tool, adapt the config section in newsplease/examples/commoncrawl.py, and execute python3 -m newsplease.examples.commoncrawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
